{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Classifier_Image_Text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONgCQNJGS5B8"
      },
      "source": [
        "#connection with google drive in case you use google colaboratory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5ENj11STGFx"
      },
      "source": [
        "#import libraries and modules\n",
        "SEED = 1234\n",
        "! pip install pyprind\n",
        "import torchtext\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import json\n",
        "import pyprind\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy import data\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "torch.backends.cudnn.deterministic = True\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "torch.cuda.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLOT_2f8TGN7"
      },
      "source": [
        "#initialize cuda to use GPU when it is available \n",
        "is_cuda = torch.cuda.is_available()\n",
        "print(\"Cuda Status on system is {}\".format(is_cuda))\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slW6L5UpTGTG"
      },
      "source": [
        "#unzip and load pt files containing image information (features and classes)\n",
        "import tarfile\n",
        "t = tarfile.open('/content/drive/coco_object_features.tar.gz', 'r')\n",
        "t.extractall(\"/content/drive/embedding\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PJSvg72TGXT"
      },
      "source": [
        "#check number of pt files loaded\n",
        "import os\n",
        "path, dirs, files = next(os.walk(\"/content/drive/coco_object_features/features\"))\n",
        "file_count = len(files)\n",
        "print(file_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4zQRy81TGc9"
      },
      "source": [
        "#load training dataset and validation dataset from google drive\n",
        "train_set = pd.read_csv(\"/content/drive/train.csv\")\n",
        "validation_set = pd.read_csv(\"/content/drive/validation.csv\")\n",
        "#mix training set records\n",
        "train_set = train_set.sample(n=train_set.shape[0])\n",
        "validation_set = validation_set.sample(n=validation_set.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqrxxysvTGh8"
      },
      "source": [
        "#load pre-trained Glove embeddings\n",
        "vocab = torchtext.vocab.Vectors(\"/content/drive/glove.6B.300d.txt\")\n",
        "#adding element for padding inside the vocabulary at position 400000\n",
        "vector_400000 = torch.zeros(300)\n",
        "vector_400000.view(1,300).shape\n",
        "vocab.vectors = torch.cat((vocab.vectors, vector_400000.view(1,300)), dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSn51e1UTGmp"
      },
      "source": [
        "#create the required dataset class for dataloder\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self,dataframe):\n",
        "        self.samples = dataframe\n",
        "    #the size of the dataset (we can reduce it when needed Ex: return 28 for 28 lines)\n",
        "    def __len__(self): \n",
        "      return len(self.samples)\n",
        "      \n",
        "    def __getitem__(self, idx):\n",
        "      '''\n",
        "      return current elements of the dataset(at index idx)\n",
        "      '''\n",
        "      #get caption\n",
        "      word_idx = []\n",
        "      caption = self.samples.caption.iloc[idx].lower()\n",
        "      row = nltk.word_tokenize(caption)\n",
        "      #get indexes of word's caption from embedding ('vocab')\n",
        "      for word in row:\n",
        "        try :\n",
        "          index = vocab.stoi[word]\n",
        "          word_idx.append(index)\n",
        "        except KeyError:\n",
        "          word_idx.append(400000)\n",
        "      #padding to get 13 words from a caption\n",
        "      while len(word_idx) <13:\n",
        "        word_idx.append(400000)\n",
        "\n",
        "      #get the current image feature\n",
        "      image_id = self.samples.iloc[idx].image_id\n",
        "      file_image =\"/content/drive/coco_object_features/features/\"+'0' * (12-len(str(image_id))) + str(image_id) + '.pt'\n",
        "      images = torch.load(file_image)\n",
        "      data_classes = images['classes']\n",
        "      data_features = images['features']\n",
        "      i=0\n",
        "      #get indexes of the expected 5 classes name from embedding ('vocab')\n",
        "      class_idx=[]\n",
        "      for tag in data_classes:\n",
        "        try:\n",
        "          tag = tag.decode(\"utf-8\")\n",
        "          tag = tag.lower()\n",
        "          index = vocab.stoi[tag]\n",
        "          class_idx.append(index)\n",
        "        except KeyError:\n",
        "          class_idx.append(400000)    \n",
        "      #padding to get 5 classes\n",
        "      while len(class_idx) < 5:\n",
        "        class_idx.append(400000)\n",
        "      data_features = data_features[:5]\n",
        "      while len(data_features) < 5:\n",
        "        row = torch.zeros(1,2048)\n",
        "        data_features = torch.cat((torch.tensor(data_features), row),dim=0)\n",
        "      #get the label of the current line\n",
        "      label = self.samples.Label.iloc[idx]\n",
        "\n",
        "      #return: the 13 first words indexes, features, and the first 5 classes of the single image\n",
        "      return   torch.tensor(word_idx[:13]), torch.tensor(data_features), torch.tensor(class_idx[:5]), torch.tensor(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97Iu1RW9TGrl"
      },
      "source": [
        "#create torch Dataset for train and validation\n",
        "dataset_train = Dataset(train_set)\n",
        "dataset_validation = Dataset(validation_set)\n",
        "#use dataloder for training and validation\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=32, num_workers=2)\n",
        "dataloader_validation = DataLoader(dataset_validation, batch_size=32, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMj0HIM7TuHJ"
      },
      "source": [
        "#set two recurrent neural network(RNN) and a linear classifier\n",
        "class CRM(nn.Module): #cross retrieval match\n",
        "  def __init__(self, vocab_dim, embedding_language_dim, embedding_classes_dim, embedding_features_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        #RNN for caption processing\n",
        "        self.embeddings_language = nn.Embedding(vocab_dim, embedding_language_dim) \n",
        "        self.embeddings_language.load_state_dict({'weight': (vocab.vectors)})\n",
        "        self.rnn_language = nn.RNN(embedding_language_dim, hidden_dim, batch_first = True)\n",
        "        \n",
        "        #RNN for images processing\n",
        "        embedding_img_dim = embedding_classes_dim + embedding_features_dim\n",
        "        self.embeddings_visual = nn.Embedding(vocab_dim, embedding_classes_dim)\n",
        "        self.embeddings_visual.load_state_dict({'weight': (vocab.vectors)})\n",
        "        self.rnn_visual = nn.RNN(embedding_img_dim, hidden_dim, batch_first = True)\n",
        "         \n",
        "        #turn off somes neurons avoiding overfitting \n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        #linear classifier\n",
        "        self.fc = nn.Linear(hidden_dim + hidden_dim, output_dim)\n",
        "\n",
        "        #forward get in input dataloader output: captions indexes, features, classes indexes\n",
        "  def forward(self, caption_idx, embedding_features, classes_idx):\n",
        "          '''\n",
        "          Forwarding method: give as a input a tensor which summarize the current image and caption\n",
        "          '''\n",
        "          #load words embeddings for captions\n",
        "          embedding_text = self.embeddings_language(caption_idx)\n",
        "          _,hidden_language = self.rnn_language(embedding_text) #saving only the last hidden which summarize the caption\n",
        "          \n",
        "          #load words embeddings for image classes \n",
        "          embedding_classes = self.embeddings_visual(classes_idx)\n",
        "          #concatenate image features and classes\n",
        "          embedding_image = torch.cat((embedding_features, embedding_classes), dim=-1)\n",
        "          #saving only the last hidden which summarize the image features\n",
        "          _, hidden_visual = self.rnn_visual(embedding_image)\n",
        "          \n",
        "          #tensor that summarize both image and caption\n",
        "          hidden = torch.cat((hidden_language, hidden_visual), dim = -1)\n",
        "          \n",
        "\n",
        "          out = self.fc(hidden)\n",
        "          out = self.dropout(out) #avoid overfitting\n",
        "\n",
        "          return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-dmVHKwpBln"
      },
      "source": [
        "#declare model\n",
        "VOCAB_DIM = len(vocab.vectors)\n",
        "EMBEDDING_CLASSES_DIM = 300\n",
        "EMBEDDING_FEATURES_DIM = 2048\n",
        "EMBEDDING_LANGUAGE_DIM = 300\n",
        "HIDDEN_DIM = 450\n",
        "OUTPUT_DIM = 2\n",
        "model = CRM(VOCAB_DIM, EMBEDDING_LANGUAGE_DIM, EMBEDDING_CLASSES_DIM, EMBEDDING_FEATURES_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_NaAtzvK0C2"
      },
      "source": [
        "#take weights to GPU\n",
        "class_weights = torch.tensor([1.0, 1.0]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CErnejYtLD-z"
      },
      "source": [
        "#choose parameters for learning\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-6,weight_decay=12e-7)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea_kSzvQpBuw"
      },
      "source": [
        "#loading model and criterion to the device ('GPU' o 'CPU')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZiBci4rFOWT"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "\n",
        "    \"\"\"\n",
        "    preds, ind= torch.max(F.softmax(preds, dim=-1), 1)\n",
        "    correct = (ind == y).float()\n",
        "    correct = correct\n",
        "    acc = correct.sum()/float(len(correct))\n",
        "\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaJ0B7X7Uay1"
      },
      "source": [
        "#List to memorize noises while the model makes predictions\n",
        "loss_by_step=[]\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    \"\"\"\n",
        "    function to train model\n",
        "\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train() #launch the dropout\n",
        "    bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
        "    for batch in iterator:\n",
        "      optimizer.zero_grad()\n",
        "      #forward propagation with parameters as required by the forward method\n",
        "      prediction = model(batch[0].to(device), batch[1].to(device), batch[2].to(device)).squeeze(0)\n",
        "\n",
        "      loss = criterion(prediction, batch[3].to(device))\n",
        "      acc = binary_accuracy(prediction, batch[3].to(device)).to(device)\n",
        "        \n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      loss_by_step.append(loss.item())\n",
        "      bar.update() \n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "      bar.update()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqfAORMp9-6j"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    '''\n",
        "    function for evaluating model\n",
        "\n",
        "    '''\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval() #stop dropout\n",
        "    \n",
        "    with torch.no_grad(): #stop to train model\n",
        "        bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch[0].to(device), batch[1].to(device), batch[2].to(device)).squeeze(0)\n",
        "            \n",
        "            loss = criterion(predictions, batch[3].to(device)) #add weight decay (normalize weights)\n",
        "            acc = binary_accuracy(predictions, batch[3].to(device))\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            bar.update()\n",
        "            \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffAgVIPj1kel"
      },
      "source": [
        "#yield scores for visual representation\n",
        "train_loss_=[]\n",
        "train_acc_=[]\n",
        "val_loss_=[]\n",
        "val_acc_=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT8LlyONpB1Y"
      },
      "source": [
        "#train and validate model\n",
        "\n",
        "N_EPOCHS = 1\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss, train_acc = train(model, dataloader_train, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, dataloader_validation, criterion)\n",
        "\n",
        "    train_loss_.append(f'{train_loss:.3f}')\n",
        "    train_acc_.append(f'{train_acc*100:.2f}')\n",
        "    val_loss_.append(f'{valid_loss:.3f}')\n",
        "    val_acc_.append(f'{valid_acc*100:.2f}')\n",
        "    graph = pd.DataFrame(train_loss_)\n",
        "    graph = graph.rename(columns={0:'train_loss_'})\n",
        "    graph['train_acc_']= train_acc_\n",
        "    graph['val_loss_']=val_loss_\n",
        "    graph['val_acc_']=val_acc_\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHwM8wgykCP7"
      },
      "source": [
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVm_tq1sxvn1"
      },
