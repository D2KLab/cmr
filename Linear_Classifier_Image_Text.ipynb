{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Classifier_Image_Text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONgCQNJGS5B8"
      },
      "source": [
        "#connection with google drive in case you use google colaboratory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5ENj11STGFx"
      },
      "source": [
        "#import libraries and modules\n",
        "SEED = 1234\n",
        "! pip install pyprind\n",
        "import torchtext\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import json\n",
        "import pyprind\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy import data\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "torch.backends.cudnn.deterministic = True\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "torch.cuda.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLOT_2f8TGN7"
      },
      "source": [
        "#initialize cuda to use GPU when it is available \n",
        "is_cuda = torch.cuda.is_available()\n",
        "print(\"Cuda Status on system is {}\".format(is_cuda))\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slW6L5UpTGTG"
      },
      "source": [
        "#unzip and load pt files containing image information (features and classes)\n",
        "import tarfile\n",
        "t = tarfile.open('/content/drive/coco_object_features.tar.gz', 'r')\n",
        "t.extractall(\"/content/drive/embedding\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PJSvg72TGXT"
      },
      "source": [
        "#check number of pt files loaded\n",
        "import os\n",
        "path, dirs, files = next(os.walk(\"/content/drive/coco_object_features/features\"))\n",
        "file_count = len(files)\n",
        "print(file_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4zQRy81TGc9"
      },
      "source": [
        "#load training dataset and validation dataset from google drive\n",
        "train_set = pd.read_csv(\"/content/drive/train.csv\")\n",
        "validation_set = pd.read_csv(\"/content/drive/validation.csv\")\n",
        "#mix training set records\n",
        "train_set = train_set.sample(n=train_set.shape[0])\n",
        "validation_set = validation_set.sample(n=validation_set.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqrxxysvTGh8"
      },
      "source": [
        "#load pre-trained Glove embeddings\n",
        "vocab = torchtext.vocab.Vectors(\"/content/drive/glove.6B.300d.txt\")\n",
        "#adding element for padding inside the vocabulary at position 400000\n",
        "vector_400000 = torch.zeros(300)\n",
        "vector_400000.view(1,300).shape\n",
        "vocab.vectors = torch.cat((vocab.vectors, vector_400000.view(1,300)), dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSn51e1UTGmp"
      },
      "source": [
        "#create the required dataset class for dataloder\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self,dataframe):\n",
        "        self.samples = dataframe\n",
        "    #the size of the dataset (we can reduce it when needed Ex: return 28 for 28 lines)\n",
        "    def __len__(self): \n",
        "      return len(self.samples)\n",
        "      \n",
        "    def __getitem__(self, idx):\n",
        "      '''\n",
        "      return current elements of the dataset(at index idx)\n",
        "      '''\n",
        "      #get caption\n",
        "      word_idx = []\n",
        "      caption = self.samples.caption.iloc[idx].lower()\n",
        "      row = nltk.word_tokenize(caption)\n",
        "      #get indexes of word's caption from embedding ('vocab')\n",
        "      for word in row:\n",
        "        try :\n",
        "          index = vocab.stoi[word]\n",
        "          word_idx.append(index)\n",
        "        except KeyError:\n",
        "          word_idx.append(400000)\n",
        "      #padding to get 13 words from a caption\n",
        "      while len(word_idx) <13:\n",
        "        word_idx.append(400000)\n",
        "\n",
        "      #get the current image feature\n",
        "      image_id = self.samples.iloc[idx].image_id\n",
        "      file_image =\"/content/drive/coco_object_features/features/\"+'0' * (12-len(str(image_id))) + str(image_id) + '.pt'\n",
        "      images = torch.load(file_image)\n",
        "      data_classes = images['classes']\n",
        "      data_features = images['features']\n",
        "      i=0\n",
        "      #get indexes of the expected 5 classes name from embedding ('vocab')\n",
        "      class_idx=[]\n",
        "      for tag in data_classes:\n",
        "        try:\n",
        "          tag = tag.decode(\"utf-8\")\n",
        "          tag = tag.lower()\n",
        "          index = vocab.stoi[tag]\n",
        "          class_idx.append(index)\n",
        "        except KeyError:\n",
        "          class_idx.append(400000)    \n",
        "      #padding to get 5 classes\n",
        "      while len(class_idx) < 5:\n",
        "        class_idx.append(400000)\n",
        "      data_features = data_features[:5]\n",
        "      while len(data_features) < 5:\n",
        "        row = torch.zeros(1,2048)\n",
        "        data_features = torch.cat((torch.tensor(data_features), row),dim=0)\n",
        "      #get the label of the current line\n",
        "      label = self.samples.Label.iloc[idx]\n",
        "\n",
        "      #return: the 13 first words indexes, features, and the first 5 classes of the single image\n",
        "      return   torch.tensor(word_idx[:13]), torch.tensor(data_features), torch.tensor(class_idx[:5]), torch.tensor(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97Iu1RW9TGrl"
      },
      "source": [
        "#create torch Dataset for train and validation\n",
        "dataset_train = Dataset(train_set)\n",
        "dataset_validation = Dataset(validation_set)\n",
        "#use dataloder for training and validation\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=32, num_workers=2)\n",
        "dataloader_validation = DataLoader(dataset_validation, batch_size=32, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMj0HIM7TuHJ"
      },
      "source": [
        "#set two recurrent neural network(RNN) and a linear classifier\n",
        "class CRM(nn.Module): #cross retrieval match\n",
        "  def __init__(self, vocab_dim, embedding_language_dim, embedding_classes_dim, embedding_features_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        #RNN for caption processing\n",
        "        self.embeddings_language = nn.Embedding(vocab_dim, embedding_language_dim) \n",
        "        self.embeddings_language.load_state_dict({'weight': (vocab.vectors)})\n",
        "        self.rnn_language = nn.RNN(embedding_language_dim, hidden_dim, batch_first = True)\n",
        "        \n",
        "        #RNN for images processing\n",
        "        embedding_img_dim = embedding_classes_dim + embedding_features_dim\n",
        "        self.embeddings_visual = nn.Embedding(vocab_dim, embedding_classes_dim)\n",
        "        self.embeddings_visual.load_state_dict({'weight': (vocab.vectors)})\n",
        "        self.rnn_visual = nn.RNN(embedding_img_dim, hidden_dim, batch_first = True)\n",
        "         \n",
        "        #turn off somes neurons avoiding overfitting \n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        #linear classifier\n",
        "        self.fc = nn.Linear(hidden_dim + hidden_dim, output_dim)\n",
        "\n",
        "        #forward get in input dataloader output: captions indexes, features, classes indexes\n",
        "  def forward(self, caption_idx, embedding_features, classes_idx):\n",
        "          '''\n",
        "          Forwarding method: give as a input a tensor which summarize the current image and caption\n",
        "          '''\n",
        "          #load words embeddings for captions\n",
        "          embedding_text = self.embeddings_language(caption_idx)\n",
        "          _,hidden_language = self.rnn_language(embedding_text) #saving only the last hidden which summarize the caption\n",
        "          \n",
        "          #load words embeddings for image classes \n",
        "          embedding_classes = self.embeddings_visual(classes_idx)\n",
        "          #concatenate image features and classes\n",
        "          embedding_image = torch.cat((embedding_features, embedding_classes), dim=-1)\n",
        "          #saving only the last hidden which summarize the image features\n",
        "          _, hidden_visual = self.rnn_visual(embedding_image)\n",
        "          \n",
        "          #tensor that summarize both image and caption\n",
        "          hidden = torch.cat((hidden_language, hidden_visual), dim = -1)\n",
        "          \n",
        "\n",
        "          out = self.fc(hidden)\n",
        "          out = self.dropout(out) #avoid overfitting\n",
        "\n",
        "          return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dJZee1uTuNp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rG6A6ciTuVL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT7_1208Tud5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX3u8da6Tui-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht40sCcJTunG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}